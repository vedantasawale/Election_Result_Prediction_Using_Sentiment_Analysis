{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Relevant Libraries and Load Raw Collected Tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "dataset = pd.read_csv('politicaltweetsmany.csv')\n",
    "corpus = []\n",
    "sentiments= []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocess Tweets,remove noise and create a Corpus</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "flag=0\n",
    "for i in range(0,75001):\n",
    "    tweet=dataset['text'][i]\n",
    "    #print(tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "   # print(tweet)\n",
    "#removing  emojis and n\n",
    "    tweet = re.sub(r\"#[a-zA-Z0-9]+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\\\x[a-zA-Z0-9]+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\\\n\", \" \", tweet)\n",
    " \n",
    "    \n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    \n",
    "    \n",
    "    tweet = re.sub(r\"\\W\",\" \",tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"\\d\",\" \",tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"s+[a-z]\\s+\",\" \",tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"\\s+[a-z]$\",\" \",tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"^[a-z]\\s+\",\" \",tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"\\s+\",\" \",tweet)\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.split()\n",
    "    #Stemming \n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
    "    #tweet = [lemmatizer.lemmatize(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
    "    #print(tweet)\n",
    "    tweet = ' '.join(tweet)\n",
    "    \n",
    "    score = TextBlob(tweet)\n",
    "    #compound=score['compound']\n",
    "    #print(\"score=\",score)\n",
    "   # print(a.sentiment.polarity)\n",
    "    sent=0\n",
    "    if(score.sentiment.polarity<0):\n",
    "        sent=0\n",
    "    elif(score.sentiment.polarity>0):\n",
    "        sent=1\n",
    "    \n",
    "    \n",
    "        \n",
    "    if(score.sentiment.polarity!=0):\n",
    "        \n",
    "        sentiments.append(sent)\n",
    "    #print(dataset['text'][i],\"sentiment=\",sent)      \n",
    "        corpus.append(tweet)\n",
    "        y.append(sent)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Extraction and Training Different models to find the best model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(max_features=700)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#y = datasset.iloc[:, 1].values\n",
    "\n",
    "#splitting the dtaset into the trning set and testset\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vedant/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1375  240]\n",
      " [1010 4840]]\n",
      "0.8325519089082385\n"
     ]
    }
   ],
   "source": [
    "#fitting Naivebayes to the training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#mking confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm =  confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Tree</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1459  156]\n",
      " [ 190 5660]]\n",
      "0.9536503683858004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(max_features=2000)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model=DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#mking confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm =  confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SVM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1265  350]\n",
      " [ 124 5726]]\n",
      "0.936503683858004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(max_features=700)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#y = datasset.iloc[:, 1].values\n",
    "\n",
    "#splitting the dtaset into the trning set and testset\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(X_train, y_train)  \n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm =  confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 23376 n= 6480\n",
      "p= 5850 n= 1615\n",
      "p= 5816 n= 1649\n"
     ]
    }
   ],
   "source": [
    "p=0\n",
    "n=0\n",
    "for i in y_train:\n",
    "    if(i==1):\n",
    "        p=p+1\n",
    "    elif(i==0):\n",
    "        n=n+1\n",
    "print(\"p=\",p,\"n=\",n)       \n",
    "p=0\n",
    "n=0\n",
    "for i in y_test:\n",
    "    if(i==1):\n",
    "        p=p+1\n",
    "    elif(i==0):\n",
    "        n=n+1\n",
    "print(\"p=\",p,\"n=\",n)       \n",
    "\n",
    "p=0\n",
    "n=0\n",
    "for i in y_pred:\n",
    "    if(i==1):\n",
    "        p=p+1\n",
    "    elif(i==0):\n",
    "        n=n+1\n",
    "print(\"p=\",p,\"n=\",n)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Store Preprocessed tweets in File for Future Use</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "csvFile = open('preprocessedtweets2.csv', 'a')\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow([\"tweet\",\"sentiment\"])\n",
    "# minimum among all the three  \n",
    "for (tweet, sentiment) in zip(corpus,y): \n",
    "     csvWriter.writerow([tweet,sentiment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Random Forest</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessedtweetsfinal.csv')\n",
    "corpus=data['tweet']\n",
    "y=data['sentiment']\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "#y = datasset.iloc[:, 1].values\n",
    "\n",
    "#splitting the dtaset into the trning set and testset\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#mking confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm =  confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
